{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cccb7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "import gc\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "\n",
    "from util import (load_video, load_background, load_mask, metric_CIEDE2000, evaluate, load_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37045422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame, frame_size=(720, 1280)):\n",
    "    \"\"\"\n",
    "    Shared preprocessing function for both training and testing\n",
    "    \"\"\"\n",
    "    frame_resized = cv2.resize(frame, frame_size)\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    return frame_rgb\n",
    "\n",
    "def frame_to_tensor(frame_rgb):\n",
    "    \"\"\"\n",
    "    Convert preprocessed frame to tensor (same as training)\n",
    "    \"\"\"\n",
    "    return torch.FloatTensor(frame_rgb).permute(2, 0, 1) / 255.0\n",
    "\n",
    "\n",
    "\n",
    "class VideoFrameDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Memory-efficient dataset that loads video frames on-demand[4][8]\n",
    "    \"\"\"\n",
    "    def __init__(self, triplets, frame_size=(720, 1280), \n",
    "                 frames_per_video=50, similarity_method='combined', \n",
    "                 mode='train'):  # Add mode parameter\n",
    "        self.triplets = triplets\n",
    "        self.frame_size = frame_size\n",
    "        self.frames_per_video = frames_per_video\n",
    "        self.similarity_method = similarity_method\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode == 'test':\n",
    "            # For testing, we don't build frame indices since we'll process complete videos\n",
    "            self.frame_indices = []\n",
    "        else:\n",
    "            # For training/validation, build frame indices as before\n",
    "            self.frame_indices = []\n",
    "            self._build_frame_index()\n",
    "\n",
    "    def _build_frame_index(self):\n",
    "        \"\"\"Build index of all available frames across videos\"\"\"\n",
    "        for triplet_idx, (video_path, mask_path, bg_path) in enumerate(self.triplets):\n",
    "            # Get video info without loading all frames[3]\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            cap.release()\n",
    "            \n",
    "            # Sample frame indices evenly across video[8]\n",
    "            if total_frames > self.frames_per_video:\n",
    "                frame_step = total_frames // self.frames_per_video\n",
    "                selected_frames = list(range(0, total_frames, frame_step))[:self.frames_per_video]\n",
    "            else:\n",
    "                selected_frames = list(range(total_frames))\n",
    "            \n",
    "            # Add to global frame index\n",
    "            for frame_idx in selected_frames:\n",
    "                self.frame_indices.append((triplet_idx, frame_idx)) # type: ignore\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.mode == 'test':\n",
    "            return len(self.triplets)  # One sample per video triplet\n",
    "        else:\n",
    "            return len(self.frame_indices) # type: list[Tuple[int, int]]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'test':\n",
    "            # Return video paths for complete reconstruction\n",
    "            video_path, mask_path, bg_path = self.triplets[idx]\n",
    "            return {\n",
    "                'video_path': str(video_path),\n",
    "                'background_path': str(bg_path),\n",
    "                'mask_path': str(mask_path)\n",
    "            }\n",
    "        else:\n",
    "            # Normal frame-by-frame loading for training\n",
    "            triplet_idx, frame_idx = self.frame_indices[idx]\n",
    "            video_path, mask_path, bg_path = self.triplets[triplet_idx]\n",
    "            \n",
    "            frame = self._load_single_frame(video_path, frame_idx)\n",
    "            background = self._load_background(bg_path)\n",
    "            similarity_mask = create_continuous_similarity_mask(\n",
    "                frame, background, method=self.similarity_method\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'frame': frame_to_tensor(frame),  # Consistent tensor conversion\n",
    "                'similarity_mask': torch.FloatTensor(similarity_mask).unsqueeze(0),\n",
    "                'background': frame_to_tensor(background)  # Consistent tensor conversion\n",
    "            }\n",
    "\n",
    "    \n",
    "    def _load_single_frame(self, video_path: Path, frame_idx: int) -> np.ndarray:\n",
    "        \"\"\"Load single frame from video efficiently\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if not ret:\n",
    "            raise ValueError(f\"Could not read frame {frame_idx} from {video_path}\")\n",
    "        \n",
    "        return preprocess_frame(frame, self.frame_size)\n",
    "    \n",
    "    def _load_background(self, bg_path: Path) -> np.ndarray:\n",
    "        \"\"\"Load and cache background images\"\"\"\n",
    "        if not hasattr(self, '_bg_cache'):\n",
    "            self._bg_cache = {}\n",
    "        \n",
    "        if str(bg_path) not in self._bg_cache:\n",
    "            bg = cv2.imread(str(bg_path))\n",
    "            if bg is None:\n",
    "                raise ValueError(f\"Could not read background image from {bg_path}\")\n",
    "            bg_processed = preprocess_frame(bg, self.frame_size)\n",
    "            self._bg_cache[str(bg_path)] = bg_processed\n",
    "        \n",
    "        return self._bg_cache[str(bg_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db69ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackgroundLeakDataModule(L.LightningDataModule):\n",
    "    \"\"\"Lightning Data Module with efficient video loading[1][2]\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 backgrounds_dir: str,\n",
    "                 videos_dir: str, \n",
    "                 masks_dir: str,\n",
    "                 batch_size: int = 8,  # Smaller batch for memory efficiency\n",
    "                 num_workers: int = 22, # adjusted for my laptop\n",
    "                 frames_per_video: int = 200,\n",
    "                 val_split: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.backgrounds_dir = backgrounds_dir\n",
    "        self.videos_dir = videos_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.frames_per_video = frames_per_video\n",
    "        self.val_split = val_split\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Called only once to prepare data[1]\"\"\"\n",
    "        # Verify all files exist\n",
    "        triplets = load_triplets(self.backgrounds_dir, self.videos_dir, self.masks_dir)\n",
    "        print(f\"Found {len(triplets)} video triplets\")\n",
    "        \n",
    "    def setup(self, stage: str = None): # type: ignore\n",
    "        triplets = load_triplets(self.backgrounds_dir, self.videos_dir, self.masks_dir)\n",
    "        random.shuffle(triplets)\n",
    "        split_idx = int(len(triplets) * (1 - self.val_split))\n",
    "        \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = VideoFrameDataset(\n",
    "                triplets[:split_idx],\n",
    "                frames_per_video=self.frames_per_video,\n",
    "                mode='train'\n",
    "            )\n",
    "            self.val_dataset = VideoFrameDataset(\n",
    "                triplets[split_idx:],\n",
    "                frames_per_video=self.frames_per_video // 2,\n",
    "                mode='train'  # Validation still uses frame-by-frame\n",
    "            )\n",
    "            \n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = VideoFrameDataset(\n",
    "                triplets[split_idx:],\n",
    "                mode='test'  # Test mode for complete video reconstruction\n",
    "            )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,  # Faster GPU transfer[2]\n",
    "            persistent_workers=True  # Keep workers alive[1]\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers // 2,  # Fewer workers for validation\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=1,  # Process one video at a time\n",
    "            shuffle=False,\n",
    "            num_workers=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a44e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ContinuousRegressionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss function for continuous similarity targets.\n",
    "    Supports multiple loss types optimized for soft label regression[1][4].\n",
    "    \"\"\"\n",
    "    def __init__(self, loss_type='combined', alpha=2.0, beta=1.0, gamma=0.5):\n",
    "        super().__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.alpha = alpha  # Weight for primary loss\n",
    "        self.beta = beta    # Weight for gradient loss\n",
    "        self.gamma = gamma  # Weight for structural loss\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: Model output [B, 1, H, W] in range [0, 1]\n",
    "            targets: Continuous similarity masks [B, 1, H, W] in range [0, 1]\n",
    "        \"\"\"\n",
    "        if self.loss_type == 'mse':\n",
    "            return F.mse_loss(predictions, targets)\n",
    "        \n",
    "        elif self.loss_type == 'mae':\n",
    "            # Mean Absolute Error - robust to outliers[5]\n",
    "            return F.l1_loss(predictions, targets)\n",
    "        \n",
    "        elif self.loss_type == 'smooth_l1':\n",
    "            # Smooth L1 - less sensitive to outliers than MSE[1]\n",
    "            return F.smooth_l1_loss(predictions, targets)\n",
    "        \n",
    "        elif self.loss_type == 'focal_mse':\n",
    "            # Focal-style loss for continuous targets\n",
    "            mse = (predictions - targets) ** 2\n",
    "            # Focus more on difficult pixels (high error)\n",
    "            focal_weight = (mse + 1e-8) ** (self.alpha / 2)\n",
    "            return torch.mean(focal_weight * mse)\n",
    "        \n",
    "        elif self.loss_type == 'jaccard_soft':\n",
    "            # Soft Jaccard loss for continuous targets[3]\n",
    "            intersection = torch.sum(predictions * targets, dim=(2, 3))\n",
    "            union = torch.sum(predictions + targets - predictions * targets, dim=(2, 3))\n",
    "            jaccard = intersection / (union + 1e-8)\n",
    "            return 1 - torch.mean(jaccard)\n",
    "        \n",
    "        elif self.loss_type == 'combined':\n",
    "            # Combine multiple loss components for robust training\n",
    "            return self._combined_loss(predictions, targets)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n",
    "    \n",
    "    def _combined_loss(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Combined loss with multiple components for robust training[4]\n",
    "        \"\"\"\n",
    "        # Primary regression loss (MSE)\n",
    "        mse_loss = F.mse_loss(predictions, targets)\n",
    "        \n",
    "        # Gradient-based boundary loss\n",
    "        grad_loss = self._gradient_loss(predictions, targets)\n",
    "        \n",
    "        # Structural similarity component\n",
    "        ssim_loss = self._ssim_loss(predictions, targets)\n",
    "        \n",
    "        # Weighted combination\n",
    "        total_loss = (self.alpha * mse_loss + \n",
    "                     self.beta * grad_loss + \n",
    "                     self.gamma * ssim_loss)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def _gradient_loss(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Gradient-based loss to preserve edge information[4]\n",
    "        \"\"\"\n",
    "        # Sobel operators for gradient computation\n",
    "        sobel_x = torch.FloatTensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).view(1, 1, 3, 3)\n",
    "        sobel_y = torch.FloatTensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]).view(1, 1, 3, 3)\n",
    "        \n",
    "        if predictions.is_cuda:\n",
    "            sobel_x = sobel_x.cuda()\n",
    "            sobel_y = sobel_y.cuda()\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_pred_x = F.conv2d(predictions, sobel_x, padding=1)\n",
    "        grad_pred_y = F.conv2d(predictions, sobel_y, padding=1)\n",
    "        grad_target_x = F.conv2d(targets, sobel_x, padding=1)\n",
    "        grad_target_y = F.conv2d(targets, sobel_y, padding=1)\n",
    "        \n",
    "        # Gradient magnitude loss\n",
    "        grad_loss = (F.mse_loss(grad_pred_x, grad_target_x) + \n",
    "                    F.mse_loss(grad_pred_y, grad_target_y))\n",
    "        \n",
    "        return grad_loss\n",
    "    \n",
    "    def _ssim_loss(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Structural Similarity loss component[4]\n",
    "        \"\"\"\n",
    "        # Window size for local comparison\n",
    "        window_size = 11\n",
    "        sigma = 1.5\n",
    "        \n",
    "        # Create Gaussian window\n",
    "        coords = torch.arange(window_size).float() - window_size // 2\n",
    "        g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
    "        g = g / g.sum()\n",
    "        window = g.outer(g).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        if predictions.is_cuda:\n",
    "            window = window.cuda()\n",
    "        \n",
    "        # Compute local means\n",
    "        mu1 = F.conv2d(predictions, window, padding=window_size//2, groups=1)\n",
    "        mu2 = F.conv2d(targets, window, padding=window_size//2, groups=1)\n",
    "        \n",
    "        mu1_sq = mu1 ** 2\n",
    "        mu2_sq = mu2 ** 2\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "        \n",
    "        # Compute local variances and covariance\n",
    "        sigma1_sq = F.conv2d(predictions ** 2, window, padding=window_size//2, groups=1) - mu1_sq\n",
    "        sigma2_sq = F.conv2d(targets ** 2, window, padding=window_size//2, groups=1) - mu2_sq\n",
    "        sigma12 = F.conv2d(predictions * targets, window, padding=window_size//2, groups=1) - mu1_mu2\n",
    "        \n",
    "        # SSIM constants\n",
    "        c1 = 0.01 ** 2\n",
    "        c2 = 0.03 ** 2\n",
    "        \n",
    "        # Compute SSIM\n",
    "        ssim = ((2 * mu1_mu2 + c1) * (2 * sigma12 + c2)) / \\\n",
    "               ((mu1_sq + mu2_sq + c1) * (sigma1_sq + sigma2_sq + c2))\n",
    "        \n",
    "        return 1 - torch.mean(ssim)\n",
    "\n",
    "\n",
    "\n",
    "def create_continuous_similarity_mask(frame, ground_truth_bg, method='combined'):\n",
    "    \"\"\"\n",
    "    Create continuous similarity measures instead of binary masks.\n",
    "    Returns values between 0 (dissimilar) and 1 (very similar).\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    # Convert to different color spaces for robust comparison\n",
    "    frame_lab = cv2.cvtColor(frame, cv2.COLOR_RGB2LAB).astype(np.float32)\n",
    "    bg_lab = cv2.cvtColor(ground_truth_bg, cv2.COLOR_RGB2LAB).astype(np.float32)\n",
    "    \n",
    "    if method == 'perceptual_distance':\n",
    "        # Perceptual distance in LAB space (normalized)\n",
    "        diff_lab = np.linalg.norm(frame_lab - bg_lab, axis=2)\n",
    "        similarity = np.exp(-diff_lab / 50.0)  # Exponential decay\n",
    "        \n",
    "    elif method == 'combined':\n",
    "        # LAB distance component\n",
    "        diff_lab = np.linalg.norm(frame_lab - bg_lab, axis=2)\n",
    "        sim_lab = np.exp(-diff_lab / 40.0)\n",
    "        \n",
    "        # HSV distance component\n",
    "        frame_hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "        bg_hsv = cv2.cvtColor(ground_truth_bg, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "        diff_hsv = np.linalg.norm(frame_hsv[:,:,:2] - bg_hsv[:,:,:2], axis=2)\n",
    "        sim_hsv = np.exp(-diff_hsv / 30.0)\n",
    "        \n",
    "        # Weighted combination\n",
    "        similarity = 0.6 * sim_lab + 0.4 * sim_hsv\n",
    "        \n",
    "    # Apply soft morphological operations to reduce noise\n",
    "    similarity = cv2.GaussianBlur(similarity.astype(np.float32), (5, 5), 1.0)\n",
    "    \n",
    "    return np.clip(similarity, 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5348122",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e453ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackgroundLeakSegmenter(L.LightningModule):\n",
    "    \"\"\"Lightning module for background leak segmentation[6]\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 learning_rate: float = 1e-4,\n",
    "                 loss_type: str = 'combined',\n",
    "                 model_architecture: str = 'unet',\n",
    "                 height: int = 720,\n",
    "                 width: int = 1280):\n",
    "        super().__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        # Now properly instantiated\n",
    "        self.loss_fn = ContinuousRegressionLoss(\n",
    "            loss_type=loss_type,\n",
    "            alpha=1.0,  # Weight for MSE\n",
    "            beta=0.5,   # Weight for gradient loss\n",
    "            gamma=0.3   # Weight for SSIM loss\n",
    "        )\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self._build_model(model_architecture)\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_mae = []\n",
    "        self.val_mae = []\n",
    "        \n",
    "    def _build_model(self, architecture: str):\n",
    "        \"\"\"Build segmentation model\"\"\"\n",
    "        if architecture == 'unet':\n",
    "            return self._build_unet()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown architecture: {architecture}\")\n",
    "    \n",
    "    def _build_unet(self):\n",
    "        \"\"\"Lightweight U-Net for memory efficiency\"\"\"\n",
    "        return nn.Sequential(\n",
    "            # Encoder\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Bottleneck\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Decoder\n",
    "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, 2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Output\n",
    "            nn.Conv2d(32, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        frames = batch['frame']\n",
    "        similarity_masks = batch['similarity_mask']\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = self.forward(frames)\n",
    "        loss = self.loss_fn(predictions, similarity_masks)\n",
    "        \n",
    "        # Calculate MAE for monitoring\n",
    "        mae = torch.mean(torch.abs(predictions - similarity_masks))\n",
    "        self.train_mae.append(mae.item())\n",
    "        \n",
    "        # Log metrics[2]\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_mae', mae, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        frames = batch['frame']\n",
    "        similarity_masks = batch['similarity_mask']\n",
    "        \n",
    "        predictions = self.forward(frames)\n",
    "        loss = self.loss_fn(predictions, similarity_masks)\n",
    "        mae = torch.mean(torch.abs(predictions - similarity_masks))\n",
    "        \n",
    "        self.val_mae.append(mae.item())\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_mae', mae, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, #verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "            },\n",
    "        }\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "            \"\"\"\n",
    "            Modified test step that reconstructs complete backgrounds from videos\n",
    "            \"\"\"\n",
    "            # Get the video path from the batch (you'll need to modify dataset to include this)\n",
    "            video_path = batch['video_path'][0]  # Assuming batch size 1 for testing\n",
    "            background_path = batch['background_path'][0]\n",
    "            mask_path = batch['mask_path'][0]\n",
    "\n",
    "            height = self.height\n",
    "            width = self.width\n",
    "            \n",
    "            # Reconstruct background from complete video\n",
    "            reconstructed_bg = self.reconstruct_background_from_video(\n",
    "                video_path, \n",
    "                frame_size=(height, width)\n",
    "            )\n",
    "            \n",
    "            # Load ground truth for evaluation\n",
    "            ground_truth_bg = load_background(background_path)\n",
    "            ground_truth_bg = cv2.resize(ground_truth_bg, (height, width))\n",
    "            ground_truth_bg = cv2.cvtColor(ground_truth_bg, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            evaluation_mask = load_mask(mask_path)\n",
    "            evaluation_mask = cv2.resize(evaluation_mask, (height, width))\n",
    "            \n",
    "            # Calculate CIEDE2000 metric\n",
    "            delta_e = metric_CIEDE2000(reconstructed_bg, ground_truth_bg, evaluation_mask)\n",
    "            reconstruction_score = evaluate(delta_e, evaluation_mask)\n",
    "            \n",
    "            # Log results\n",
    "            self.log('test_reconstruction_score', reconstruction_score, prog_bar=True)\n",
    "            self.log('test_delta_e_mean', float(np.mean(delta_e[evaluation_mask > 0]))) # this is the mean delta E for pixels where mask is > 0\n",
    "            \n",
    "            return {\n",
    "                'reconstruction_score': reconstruction_score,\n",
    "                'reconstructed_bg': reconstructed_bg,\n",
    "                'ground_truth_bg': ground_truth_bg\n",
    "            }\n",
    "    \n",
    "    def reconstruct_background_from_video(self, video_path, frame_size=(720, 1280)):\n",
    "        \"\"\"\n",
    "        Reconstruct background by processing all frames and keeping pixels with highest probability\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = self.device\n",
    "        \n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Cannot open video file {video_path}\")\n",
    "\n",
    "        height, width = frame_size\n",
    "        reconstruction_prob = np.zeros((height, width), dtype=np.float32)  # Store max probability per pixel\n",
    "        reconstruction_img = np.zeros((height, width, 3), dtype=np.uint8)  # Store pixel values\n",
    "\n",
    "        with torch.no_grad():\n",
    "            frame_count = 0\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Preprocess frame (SAME AS TRAINING)\n",
    "                # frame_resized = cv2.resize(frame, frame_size)\n",
    "                # frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "                frame_rgb = preprocess_frame(frame, frame_size)\n",
    "                \n",
    "                \n",
    "                # Convert to tensor and normalize (EXACTLY like in training dataset)\n",
    "                # input_tensor = torch.FloatTensor(frame_rgb).permute(2, 0, 1) / 255.0\n",
    "                # input_tensor = input_tensor.unsqueeze(0).to(device)  # Add batch dimension\n",
    "                input_tensor = frame_to_tensor(frame_rgb).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Predict\n",
    "                pred = self(input_tensor).squeeze(0).squeeze(0).cpu().numpy()  # shape: (H, W)\n",
    "\n",
    "                # Update reconstruction where prediction probability is higher\n",
    "                mask_update = pred > reconstruction_prob\n",
    "                for c in range(3):\n",
    "                    reconstruction_img[:, :, c][mask_update] = frame_rgb[:, :, c][mask_update]\n",
    "                reconstruction_prob[mask_update] = pred[mask_update]\n",
    "\n",
    "                frame_count += 1\n",
    "                if frame_count % 100 == 0:  # Progress logging\n",
    "                    print(f\"Processed {frame_count} frames...\")\n",
    "\n",
    "        cap.release()\n",
    "        print(f\"Reconstruction complete. Processed {frame_count} total frames.\")\n",
    "        print(f\"Coverage: {np.mean(reconstruction_prob > 0.1) * 100:.1f}% of pixels have prediction > 0.1\")\n",
    "        \n",
    "        return reconstruction_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57f7be",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d85fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jlb/Projects/RAID/wave2_vader/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90 video triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                     | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | loss_fn | ContinuousRegressionLoss | 0      | train\n",
      "1 | model   | Sequential               | 226 K  | train\n",
      "-------------------------------------------------------------\n",
      "226 K     Trainable params\n",
      "0         Non-trainable params\n",
      "226 K     Total params\n",
      "0.907     Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397e00bcbab14bbcac17d0b12f0ce840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4655c2a5e74953a6c2c52c08a6bccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5002bc2e0de46dea02b48333347062f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.207\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model():\n",
    "    \"\"\"Main training function with memory optimization\"\"\"\n",
    "    \n",
    "    # Initialize data module\n",
    "    data_module = BackgroundLeakDataModule(\n",
    "        backgrounds_dir='data/public/backgrounds',\n",
    "        videos_dir='data/public/videos',\n",
    "        masks_dir='data/public/masks',\n",
    "        batch_size=4,  # Small batch size for memory efficiency\n",
    "        num_workers=16,  # Adjusted for my laptop\n",
    "        frames_per_video=50  # Limit frames per video\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BackgroundLeakSegmenter(\n",
    "        learning_rate=1e-4,\n",
    "        loss_type='combined'\n",
    "    )\n",
    "    \n",
    "    # Callbacks for memory management and performance\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            verbose=True\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            monitor='val_loss',\n",
    "            save_top_k=3,\n",
    "            mode='min',\n",
    "            filename='best-{epoch}-{val_loss:.3f}'\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval='epoch'),\n",
    "    ]\n",
    "    \n",
    "    # Trainer with memory optimizations\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator='auto',  # Automatically select GPU/CPU\n",
    "        devices=1,  # Use single device to avoid memory issues\n",
    "        precision=\"16-mixed\",  # Mixed precision for memory efficiency\n",
    "        gradient_clip_val=1.0,  # Prevent exploding gradients\n",
    "        accumulate_grad_batches=4,  # Simulate larger batch size\n",
    "        callbacks=callbacks,\n",
    "        val_check_interval=0.5,  # Check validation twice per epoch\n",
    "        log_every_n_steps=10,\n",
    "        enable_progress_bar=True,\n",
    "        # REMOVED: enable_checkpointing=False,  # This was causing the conflict\n",
    "        fast_dev_run=False,  # Set to True for debugging\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    # Test model\n",
    "    trainer.test(model, data_module, ckpt_path='best')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set memory optimization flags\n",
    "    torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes\n",
    "    \n",
    "    # Enable garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a2620",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d96b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
